{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blind Source Separation (BSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import fftconvolve\n",
    "import IPython\n",
    "import pyroomacoustics as pra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blind Source Separation techniques such as Independent Vector Analysis (IVA) using an Auxiliary function are implemented in ´pyroomacoustics´. IVA based algorithms work when the number of microphones is the same as the number of sources, i.e., the determinant case. Through this example, we will deal with the case of 2 sources and 2 microphones.\n",
    "\n",
    "First, open and concatanate wav files from the CMU dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatanate audio samples to make them look long enough\n",
    "wav_files = [\n",
    "        ['../../../examples/input_samples/cmu_arctic_us_axb_a0004.wav',\n",
    "            '../../../examples/input_samples/cmu_arctic_us_axb_a0005.wav',\n",
    "            '../../../examples/input_samples/cmu_arctic_us_axb_a0006.wav',],\n",
    "        ['../../../examples/input_samples/cmu_arctic_us_aew_a0001.wav',\n",
    "            '../../../examples/input_samples/cmu_arctic_us_aew_a0002.wav',\n",
    "            '../../../examples/input_samples/cmu_arctic_us_aew_a0003.wav',]\n",
    "        ]\n",
    "\n",
    "signals = [ np.concatenate([wavfile.read(f)[1].astype(np.float32)\n",
    "        for f in source_files])\n",
    "for source_files in wav_files ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an anechoic room envrionment, as well as the microphone array and source locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room 4m by 6m\n",
    "room_dim = [8, 9]\n",
    "\n",
    "# source locations and delays\n",
    "locations = [[2.5,3], [2.5, 6]]\n",
    "delays = [1., 0.]\n",
    "\n",
    "# create an anechoic room with sources and mics\n",
    "room = pra.ShoeBox(room_dim, fs=16000, max_order=15, absorption=0.35, sigma2_awgn=1e-8)\n",
    "\n",
    "# add mic and good source to room\n",
    "# Add silent signals to all sources\n",
    "for sig, d, loc in zip(signals, delays, locations):\n",
    "    room.add_source(loc, signal=np.zeros_like(sig), delay=d)\n",
    "\n",
    "# add microphone array\n",
    "room.add_microphone_array(pra.MicrophoneArray(np.c_[[6.5, 4.49], [6.5, 4.51]], room.fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the RIRs as in the Room Impulse Response generation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute RIRs\n",
    "room.compute_rir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix the microphone recordings to simulate the observed signals by the microphone array in the frequency domain. To that end, we apply the STFT transform as explained in STFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mir_eval.separation import bss_eval_images\n",
    "\n",
    "# Record each source separately\n",
    "separate_recordings = []\n",
    "for source, signal in zip(room.sources, signals):\n",
    "\n",
    "    source.signal[:] = signal\n",
    "\n",
    "    room.simulate()\n",
    "    separate_recordings.append(room.mic_array.signals)\n",
    "\n",
    "    source.signal[:] = 0.\n",
    "separate_recordings = np.array(separate_recordings)\n",
    "\n",
    "# Mix down the recorded signals\n",
    "mics_signals = np.sum(separate_recordings, axis=0)\n",
    "\n",
    "# STFT frame length\n",
    "L = 2048\n",
    "# Observation vector in the STFT domain\n",
    "X = np.array([pra.stft(ch, L, L, transform=np.fft.rfft, zp_front=L//2, zp_back=L//2) for ch in mics_signals])\n",
    "X = np.moveaxis(X, 0, 2)\n",
    "\n",
    "# Reference signal to calculate performance of BSS\n",
    "ref = np.moveaxis(separate_recordings, 1, 2)\n",
    "SDR, SIR = [], []\n",
    "\n",
    "# Callback function to monitor the convergence of the algorithm\n",
    "def convergence_callback(Y):\n",
    "    global SDR, SIR\n",
    "    ref = np.moveaxis(separate_recordings, 1, 2)\n",
    "    y = np.array([pra.istft(Y[:,:,ch], L, L,\n",
    "            transform=np.fft.irfft, zp_front=L//2, zp_back=L//2) for ch in range(Y.shape[2])])\n",
    "    sdr, isr, sir, sar, perm = bss_eval_images(ref[:,:y.shape[1]-L//2,0], y[:,L//2:ref.shape[1]+L//2])\n",
    "    SDR.append(sdr)\n",
    "    SIR.append(sir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
